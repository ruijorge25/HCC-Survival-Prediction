{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                            **Assignment No. 2**\n",
    "# *Supervised Learning: Predicting 1 Year Survival of Patients with Hepatocellular Carcinoma*\n",
    "\n",
    "    The main goal of this assignment is to, by using supervised learning, try to predict if a patient will survive 1 year after being diagnosed with Hepatocellular Carcinoma (HCC)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Data Summary**\n",
    "\n",
    "This hepatocellular carcinoma dataset consists of patient-data from 165 former patients of Hospital and University Centre of Coimbra (Portugal). The dataset contains 49 features selected according to the EASL-EORTC (European Association for the Study of the Liver - European Organization for Research and Treatment of Cancer) Clinical Practice Guidelines. The target variable, \"Class\", is the survival of each patient at 1 year and is represented as 'Dies' and 'Lives'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **First Step -> Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Second Step -> Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from fancyimpute import KNN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s analyze our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcc_data = pd.read_csv(\"hcc_dataset.csv\")\n",
    "hcc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have categorical values in our data, so we will replace them with numerical values, doing a encoder method by hand, notice that we could use libraries to do this step, but we preferred doing this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df =pd.read_csv(\"hcc_dataset.csv\")\n",
    "\n",
    "# Replace \"Lives\" with \"1\" in all columns\n",
    "df.replace(\"Lives\", \"1\", inplace=True)\n",
    "\n",
    "# Replace \"Dies\" with \"0\" in all columns\n",
    "df.replace(\"Dies\", \"0\", inplace=True)\n",
    "\n",
    "# Replace \"Yes\" with \"1\" in all columns\n",
    "df.replace(\"Yes\", \"1\", inplace=True)\n",
    "\n",
    "# Replace \"No\" with \"0\" in all columns\n",
    "df.replace(\"No\", \"0\", inplace=True)\n",
    "\n",
    "# Replace \"True\" with \"1\" in all columns\n",
    "df.replace(\"True\", \"1\", inplace=True)\n",
    "\n",
    "# Replace \"False\" with \"0\" in all columns\n",
    "df.replace(\"False\", \"0\", inplace=True)\n",
    "\n",
    "# Replace \"Male\" with \"1\" in all columns\n",
    "df.replace(\"Male\", \"1\", inplace=True)\n",
    "\n",
    "# Replace \"Female\" with \"0\" in all columns\n",
    "df.replace(\"Female\", \"0\", inplace=True)\n",
    "\n",
    "# Replace \"Active\" with \"0\" in all columns\n",
    "df.replace(\"Active\", \"0\", inplace=True)\n",
    "\n",
    "# Replace \"Restricted\" with \"1\" in all columns\n",
    "df.replace(\"Restricted\", \"1\", inplace=True)\n",
    "\n",
    "# Replace \"Ambulatory\" with \"2\" in all columns\n",
    "df.replace(\"Ambulatory\", \"2\", inplace=True)\n",
    "\n",
    "# Replace \"Selfcare\" with \"3\" in all columns\n",
    "df.replace(\"Selfcare\", \"3\", inplace=True)\n",
    "\n",
    "# Replace \"Disabled\" with \"4\" in all columns\n",
    "df.replace(\"Disabled\", \"4\", inplace=True)\n",
    "\n",
    "# Replace \"None\" with \"1\" in all columns\n",
    "df.replace(\"None\", \"1\", inplace=True)\n",
    "\n",
    "# Replace \"Grade I/II\" with \"2\" in all columns\n",
    "df.replace(\"Grade I/II\", \"2\", inplace=True)\n",
    "\n",
    "# Replace \"Grade III/IV\" with \"3\" in all columns\n",
    "df.replace(\"Grade III/IV\", \"3\", inplace=True)\n",
    "\n",
    "# Replace \"Mild\" with \"2\" in all columns\n",
    "df.replace(\"Mild\", \"2\", inplace=True)\n",
    "\n",
    "# Replace \"Moderate/Severe\" with \"3\" in all columns\n",
    "df.replace(\"Moderate/Severe\", \"3\", inplace=True)\n",
    "\n",
    "# Write the modified DataFrame back to a CSV file\n",
    "df.to_csv(\"modified_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview data\n",
    "hcc_data = pd.read_csv(\"modified_file.csv\")\n",
    "hcc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all our categarical values were replaced, we are going to create dummy variables for gender to prevent the algorithm to mistakenly interpret these values as having a specific order or magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for gender\n",
    "dummy_gender = pd.get_dummies(hcc_data['Gender'], prefix='male', dummy_na=False)\n",
    "\n",
    "# Map 'false' to 0 and 'true' to 1\n",
    "dummy_gender = dummy_gender.astype(int)\n",
    "\n",
    "# Concatenate the dummy variables with the original DataFrame\n",
    "hcc_data = pd.concat([hcc_data, dummy_gender], axis=1)\n",
    "\n",
    "# Drop the original 'Gender' column\n",
    "hcc_data.drop(['Gender'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine dataset shape\n",
    "hcc_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset now has 51 columns! Which means our new 2 columns were created and our initial \"Gender\" column was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to see how many missing values we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcc_data_modified = hcc_data.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_summary = hcc_data_modified.isnull().sum()\n",
    "print(missing_data_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We can see that we have many missing data, let's adress that problem later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see the percentage of people who Lives and Dies, for that we are going to create a pie graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 0 to 'Dies' and 1 to 'Lives'\n",
    "hcc_data['Class'] = hcc_data['Class'].map({0: 'Dies', 1: 'Lives'})\n",
    "\n",
    "# Calculate percentage of \"Lives\" and \"Dies\"\n",
    "class_counts = hcc_data['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightcoral'], textprops={'color': 'black'})\n",
    "plt.title('Percentages of survival at 1 year', color='black')  # Set title color to black\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "61.8% of the patients in our dataset lives 1 year after being diagnosed with HCC, still 38.2% is more than 1/3 of the patients, which means that it is a dangerous disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s evaluate our data without rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values except in the 'Class' column\n",
    "hcc_data_cleaned = hcc_data_modified.dropna(subset=['Class'])\n",
    "\n",
    "# Check the unique values in the 'Class' column\n",
    "print(hcc_data_cleaned['Class'].unique())\n",
    "\n",
    "# Make sure 'Class' is categorical\n",
    "hcc_data_cleaned['Class'] = hcc_data_cleaned['Class'].astype('category')\n",
    "\n",
    "# Now, try plotting again\n",
    "sb.pairplot(hcc_data_cleaned, hue='Class')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the data that remains is very few, because almost all the features had missing values in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to drop the rows with missing values. Let's take a look at those now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ex-categorical values, we are going to fill the missing values with the most common value in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_impute = ['Symptoms','Alcohol','HBsAg','HBeAg','HBcAb','HCVAb','Cirrhosis','Endemic','Smoking','Diabetes','Obesity','Hemochro','AHT','CRI','HIV','NASH','Varices','Spleno','PHT','PVT','Metastasis','Hallmark','PS','Encephalopathy','Ascites']\n",
    "df_filled_specific = hcc_data_modified.copy()\n",
    "for column in columns_to_impute:\n",
    "    most_common_value = df[column].mode()[0]\n",
    "    df_filled_specific[column].fillna(most_common_value, inplace=True)\n",
    "df_filled_specific.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining missing values we are going to utilize KNN imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run KNN model using fancyimpute package\n",
    "from fancyimpute import KNN\n",
    "\n",
    "# Use 3 nearest rows which have a feature to fill in each row's missing features\n",
    "# it returns a np.array which I store as a pandas dataframe\n",
    "hcc_filled = pd.DataFrame(KNN(3).fit_transform(df_filled_specific))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##the column categories were removed when imputed into model, reinsert column headings\n",
    "hcc_filled.columns = df_filled_specific.columns\n",
    "hcc_filled.index = df_filled_specific.index\n",
    "hcc_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we don't have missing values anymore it's time to start the work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panda Profile Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Profile Report will help us see every correlation and detail about each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(hcc_filled, title='Pandas Profiling Report', explorative=True)\n",
    "profile.to_file(\"hcc_filled.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s see the correlation among the dataset features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic correlogram demonstrates minimal correlation among the dataset features\n",
    "corr = hcc_filled.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s create a summary of our features so it's easier to someone seeing our work to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summmary of 49 features:**\n",
    "\n",
    "Gender                       (1=Male;0=Female)\t\n",
    "\n",
    "Symptoms                     (1=Yes;0=No)\n",
    "\n",
    "Alcohol\t                     (1=Yes;0=No)\n",
    "\n",
    "Hepatitis B Surface Antigen\t (1=Yes;0=No)\n",
    "\n",
    "Hepatitis B e Antigen        (1=Yes;0=No)\n",
    "\n",
    "Hepatitis B Core Antibody    (1=Yes;0=No)\n",
    "\n",
    "Hepatitis C Virus Antibody\t (1=Yes;0=No)\n",
    "\n",
    "Cirrhosis                    (1=Yes;0=No)\n",
    "\n",
    "Endemic Countries\t         (1=Yes;0=No)\n",
    "\n",
    "Smoking                      (1=Yes;0=No)\n",
    "\n",
    "Diabetes\t                 (1=Yes;0=No)\n",
    "\n",
    "Obesity                      (1=Yes;0=No)\n",
    "\n",
    "Hemochromatosis              (1=Yes;0=No)\n",
    "\n",
    "Arterial Hypertension        (1=Yes;0=No)\n",
    "\n",
    "Chronic Renal Insufficiency\t (1=Yes;0=No)\n",
    "\n",
    "Human Immunodeficiency Virus (1=Yes;0=No)\n",
    "\n",
    "Nonalcoholic Steatohepatitis\t(1=Yes;0=No)\n",
    "\n",
    "Esophageal Varices\t\t\t\t(1=Yes;0=No)\n",
    "\n",
    "Splenomegaly\t\t\t\t\t(1=Yes;0=No)\n",
    "\n",
    "Portal Hypertension\t\t\t    (1=Yes;0=No)\t\n",
    "\n",
    "Portal Vein Thrombosis\t\t\t(1=Yes;0=No)\t\n",
    "\n",
    "Liver Metastasis\t\t\t\t(1=Yes;0=No)\t\n",
    "\n",
    "Radiological Hallmark\t\t\t(1=Yes;0=No)\t\n",
    "\n",
    "Age at diagnosis\t\t\t\t20-93\t\t\t\t\n",
    "\n",
    "Grams of Alcohol per day\t\tGrams/day\t\n",
    "\n",
    "Packs of cigarets per year\t\tPacks/year\t\t\n",
    "\n",
    "Performance Status*\t\t\t    [0,1,2,3,4,5]\t\n",
    "\n",
    "Encephalopathy degree*\t\t\t[1,2,3]\t\t\t\n",
    "\n",
    "Ascites degree*\t\t\t\t    [1,2,3]\t\t\t\n",
    "\n",
    "International Normalised Ratio*\t0.84-4.82\t\t\n",
    "\n",
    "Alpha-Fetoprotein (ng/mL)\t\tAFP\t\t\t\n",
    "\n",
    "Haemoglobin (g/dL)\t\t\t\t\n",
    "\n",
    "Mean Corpuscular Volume\t (fl)\tMCV\n",
    "\n",
    "Leukocytes(G/L)\t\t\t\t\t\n",
    "\n",
    "Platelets\t(G/L)\t\t\t\t\n",
    "\n",
    "Albumin (mg/dL)\t\t\t\t\n",
    "\n",
    "Total Bilirubin(mg/dL)\t\t\t\n",
    "\n",
    "Alanine transaminase (U/L)\t\tALT\n",
    "\n",
    "Aspartate transaminase (U/L)\tAST\t\t\t\t\n",
    "\n",
    "Gamma glutamyl transferase (U/L)GGT\t\t\t\n",
    "\n",
    "Alkaline phosphatase (U/L)\t\tALP\t\t\t\t\n",
    "\n",
    "Total Proteins (g/dL)\t\t\tTP\t\t\t\t\n",
    "\n",
    "Creatinine (mg/dL)\t\t\t\t\n",
    "\n",
    "Number of Nodules\t\t\t\t0-5\t\t\t\n",
    "\n",
    "Major dimension of nodule (cm)\t\n",
    "\n",
    "Direct Bilirubin (mg/dL)\t\t\n",
    "\n",
    "Iron\t(mcg/dL)\t\t\t\t\t\n",
    "\n",
    "Oxygen Saturation (%)\t\t\t\n",
    "\n",
    "Ferritin (ng/mL)\t\t\t\t\n",
    "\n",
    "Class Attribute\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to see our full data and look for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the 'Class' column\n",
    "print(hcc_filled['Class'].unique())\n",
    "\n",
    "# Make sure 'Class' is categorical\n",
    "hcc_filled['Class'] = hcc_filled['Class'].astype('category')\n",
    "\n",
    "# Now, try plotting again\n",
    "sb.pairplot(hcc_filled, hue='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a good amount of outliers, so our next step is to remove them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using the Isolation Forest (iForest) algorithm because it is a popular method for anomaly detection, particularly for identifying outliers in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "iso_forest.fit(hcc_filled)\n",
    "\n",
    "# Predict the anomalies\n",
    "hcc_filled['anomaly'] = iso_forest.predict(hcc_filled)\n",
    "\n",
    "# Filter the outliers\n",
    "hcc_filled_cleaned = hcc_filled[hcc_filled['anomaly'] == 1].drop(columns='anomaly')\n",
    "\n",
    "hcc_filled_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing again we notice that we still have some outliers, so this time we are going to remove them manually, but here's the catch, if we are going to remove them manually we need to have a reason behind it, so we are gonna write them as comments inside our next code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Excluding AFP values greater than 3000 ng/mL\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['AFP'] <= 3000]\n",
    "# Justification: AFP levels exceeding 3000 ng/mL are exceptionally high and likely indicate advanced or metastatic HCC, which may not be representative of the typical population.\n",
    "\n",
    "# 2. Excluding Packs_year values greater than 2000\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['Packs_year'] <= 500]\n",
    "# Justification: Packs_year values exceeding 500 are exceptionally high and suggest heavy smoking, which may not be representative of the typical population and can have a significant impact on health outcomes including HCC.\n",
    "\n",
    "# 3. Excluding grams_a_day values greater than 400\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['Grams_day'] <= 400]\n",
    "# Justification: grams_a_day values exceeding 400 are exceptionally high and suggest severe alcohol abuse, which is a significant risk factor for liver diseases including HCC. Excluding such extreme cases helps focus the analysis on typical patterns.\n",
    "\n",
    "# 4. Excluding encephalopathy values greater than 3.0\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['Encephalopathy'] <= 3.0]\n",
    "# Justification: encephalopathy values exceeding 3.0 indicate severe hepatic encephalopathy, which is a serious complication of advanced liver disease including HCC. Excluding such extreme cases helps focus the analysis on typical patterns.\n",
    "\n",
    "# 5. Excluding ALT values greater than 400 IU/L\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['ALT'] <= 400]\n",
    "# Justification: ALT levels exceeding 400 IU/L suggest severe liver damage, possibly from advanced liver disease including HCC. Excluding such extreme cases helps focus the analysis on typical patterns.\n",
    "\n",
    "# 6. Excluding ferritin values greater than 2000 ng/mL\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['Ferritin'] <= 2000]\n",
    "# Justification: ferritin levels exceeding 2000 ng/mL are exceptionally high and could indicate various health issues including liver disease. Excluding such extreme cases helps focus the analysis on typical patterns.\n",
    "\n",
    "# 7. Excluding GGT values above 1300 IU/L\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['GGT'] <= 1300]\n",
    "# Justification: GGT levels exceeding 1300 IU/L are exceptionally high and indicate severe liver dysfunction, likely due to advanced liver disease such as HCC.\n",
    "\n",
    "# 8. Excluding AST values above 500 IU/L\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['AST'] <= 500]\n",
    "# Justification: AST levels exceeding 500 IU/L are significantly elevated and suggest severe liver damage, likely due to advanced liver disease such as HCC.\n",
    "\n",
    "# 9. Excluding Total_bil values above 30 Âµmol/L\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['Total_Bil'] <= 30]\n",
    "# Justification: Total bilirubin levels exceeding 30 Âµmol/L are markedly elevated and indicate impaired liver function, likely due to advanced liver disease such as HCC.\n",
    "\n",
    "# 10. Excluding the patient with HIV\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['HIV'] != 1]\n",
    "# Justification: While HIV status may not directly relate to liver function, considering the patient with HIV along with extreme liver function test values as outliers helps ensure that the analysis focuses on typical patterns within the dataset.\n",
    "\n",
    "# 11. Excluding the patient with Dir_Bil level above 20 Âµmol/L\n",
    "hcc_filled_cleaned = hcc_filled_cleaned[hcc_filled_cleaned['Dir_Bil'] <= 20]\n",
    "# Justification: Direct bilirubin (Dir_Bil) levels exceeding 20 Âµmol/L are considered elevated and may indicate liver dysfunction or bile duct obstruction. With a Dir_Bil level of 30 Âµmol/L, this patient's level is significantly higher than the normal range, suggesting a substantial deviation from typical values. Excluding this patient helps ensure that the analysis focuses on typical patterns within the dataset and avoids skewing the results due to extreme values.\n",
    "\n",
    "# After excluding outliers, you may want to reset the index if needed\n",
    "hcc_filled_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hcc_filled_cleaned.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we removed all outliers, lets see how our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in the 'Class' column\n",
    "print(hcc_filled_cleaned['Class'].unique())\n",
    "\n",
    "# Make sure 'Class' is categorical\n",
    "hcc_filled_cleaned['Class'] = hcc_filled_cleaned['Class'].astype('category')\n",
    "\n",
    "# Now, try plotting again\n",
    "sb.pairplot(hcc_filled_cleaned, hue='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Modeling and Data Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's model our data and evaluate them using differents classifiers, different balancing methods and grid parameters for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "X = hcc_filled_cleaned.drop(columns=['Class'])\n",
    "y = hcc_filled_cleaned['Class']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(algorithm='SAMME')\n",
    "}\n",
    "\n",
    "# Define balancing methods\n",
    "balancing_methods = {\n",
    "    'None': None,\n",
    "    'Under-sampling': RandomUnderSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42)\n",
    "}\n",
    "\n",
    "# Grid parameters for each classifier\n",
    "grid_params = {\n",
    "    \"Decision Tree\": {'max_depth': [3, 5, 7, None]},\n",
    "    \"K-Nearest Neighbors\": {'n_neighbors': [3, 5, 7]},\n",
    "    \"Random Forest\": {'n_estimators': [50, 100, 200]},\n",
    "    \"Gradient Boosting\": {'n_estimators': [50, 100, 200], 'learning_rate': [0.05, 0.1, 0.5]},\n",
    "    \"AdaBoost\": {'n_estimators': [50, 100, 200], 'learning_rate': [0.05, 0.1, 0.5]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create our functions to evaluate the precision and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    return train_precision, test_precision, y_test_pred\n",
    "\n",
    "def plot_metrics(classifier_name, y_test, y_test_pred):\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "    plt.title(f\"{classifier_name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{classifier_name} (AUC = {roc_auc:.2f})')\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Decision Tree Classifier\n",
    "results1 = []\n",
    "\n",
    "for balance_name, sampler in balancing_methods.items():\n",
    "    print(f\"Evaluating Decision Tree with {balance_name} balancing method...\\n\")\n",
    "    \n",
    "    if sampler:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "\n",
    "    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid=grid_params[\"Decision Tree\"], scoring='accuracy', cv=5)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Training and evaluating on test set\n",
    "    train_precision, test_precision, y_test_pred = evaluate_classifier(best_classifier, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    plot_metrics(\"Decision Tree\", y_test, y_test_pred)\n",
    "    \n",
    "    # Performing cross-validation for precision\n",
    "    precision_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='precision_weighted')\n",
    "    mean_precision = precision_scores.mean()\n",
    "    std_precision = precision_scores.std()\n",
    "    \n",
    "    # Performing cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    mean_accuracy = accuracy_scores.mean()\n",
    "    std_accuracy = accuracy_scores.std()\n",
    "    \n",
    "    results1.append((balance_name, \"Decision Tree\", mean_precision, std_precision, mean_accuracy, std_accuracy))\n",
    "    \n",
    "    print(f\"Mean Precision (CV): {mean_precision:.4f}\")\n",
    "    print(f\"Std Precision (CV): {std_precision:.4f}\")\n",
    "    print(f\"Mean Accuracy (CV): {mean_accuracy:.4f}\")\n",
    "    print(f\"Std Accuracy (CV): {std_accuracy:.4f}\\n\")\n",
    "\n",
    "# Compiling results in a DataFrame\n",
    "results1_df = pd.DataFrame(results1, columns=['Balancing', 'Classifier', 'Mean Precision', 'Std Precision', 'Mean Accuracy', 'Std Accuracy'])\n",
    "print(results1_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ADASYN had better results with a mean accuracy of 0.722857 and a mean precision of 0.715118 compared to other balancing methods for the Decision Tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating K-Nearest Neighbors\n",
    "results2 = []\n",
    "\n",
    "for balance_name, sampler in balancing_methods.items():\n",
    "    print(f\"Evaluating K-Nearest Neighbors with {balance_name} balancing method...\\n\")\n",
    "    \n",
    "    if sampler:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid=grid_params[\"K-Nearest Neighbors\"], scoring='accuracy', cv=5)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Training and evaluating on test set\n",
    "    train_precision, test_precision, y_test_pred = evaluate_classifier(best_classifier, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    plot_metrics(\"K-Nearest Neighbors\", y_test, y_test_pred)\n",
    "    \n",
    "    # Performing cross-validation for precision\n",
    "    precision_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='precision_weighted')\n",
    "    mean_precision = precision_scores.mean()\n",
    "    std_precision = precision_scores.std()\n",
    "    \n",
    "    # Performing cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    mean_accuracy = accuracy_scores.mean()\n",
    "    std_accuracy = accuracy_scores.std()\n",
    "    \n",
    "    results2.append((balance_name, \"K-Nearest Neighbors\", mean_precision, std_precision, mean_accuracy, std_accuracy))\n",
    "    \n",
    "    print(f\"Mean Precision (CV): {mean_precision:.4f}\")\n",
    "    print(f\"Std Precision (CV): {std_precision:.4f}\")\n",
    "    print(f\"Mean Accuracy (CV): {mean_accuracy:.4f}\")\n",
    "    print(f\"Std Accuracy (CV): {std_accuracy:.4f}\\n\")\n",
    "\n",
    "# Compiling results in a DataFrame\n",
    "results2_df = pd.DataFrame(results2, columns=['Balancing', 'Classifier', 'Mean Precision', 'Std Precision', 'Mean Accuracy', 'Std Accuracy'])\n",
    "print(results2_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ADASYN had better results with a mean precision of 0.679572 and a mean accuracy of 0.670420 compared to other balancing methods for the K-Nearest Neighbors classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Random Forest\n",
    "results3 = []\n",
    "\n",
    "for balance_name, sampler in balancing_methods.items():\n",
    "    print(f\"Evaluating Random Forest with {balance_name} balancing method...\\n\")\n",
    "    \n",
    "    if sampler:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid=grid_params[\"Random Forest\"], scoring='accuracy', cv=5)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Training and evaluating on test set\n",
    "    train_precision, test_precision, y_test_pred = evaluate_classifier(best_classifier, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    plot_metrics(\"Random Forest\", y_test, y_test_pred)\n",
    "    \n",
    "    # Performing cross-validation for precision\n",
    "    precision_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='precision_weighted')\n",
    "    mean_precision = precision_scores.mean()\n",
    "    std_precision = precision_scores.std()\n",
    "    \n",
    "    # Performing cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    mean_accuracy = accuracy_scores.mean()\n",
    "    std_accuracy = accuracy_scores.std()\n",
    "    \n",
    "    results3.append((balance_name, \"Random Forest\", mean_precision, std_precision, mean_accuracy, std_accuracy))\n",
    "    \n",
    "    print(f\"Mean Precision (CV): {mean_precision:.4f}\")\n",
    "    print(f\"Std Precision (CV): {std_precision:.4f}\")\n",
    "    print(f\"Mean Accuracy (CV): {mean_accuracy:.4f}\")\n",
    "    print(f\"Std Accuracy (CV): {std_accuracy:.4f}\\n\")\n",
    "\n",
    "# Compiling results in a DataFrame\n",
    "results3_df = pd.DataFrame(results3, columns=['Balancing', 'Classifier', 'Mean Precision', 'Std Precision', 'Mean Accuracy', 'Std Accuracy'])\n",
    "print(results3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SMOTE had better results with a mean precision of 0.884804 and a mean accuracy of 0.831933 compared to other balancing methods for the Random Forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Gradient Boosting\n",
    "results4 = []\n",
    "\n",
    "for balance_name, sampler in balancing_methods.items():\n",
    "    print(f\"Evaluating Gradient Boosting with {balance_name} balancing method...\\n\")\n",
    "    \n",
    "    if sampler:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "\n",
    "    grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid=grid_params[\"Gradient Boosting\"], scoring='accuracy', cv=5)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Training and evaluating on test set\n",
    "    train_precision, test_precision, y_test_pred = evaluate_classifier(best_classifier, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    plot_metrics(\"Gradient Boosting\", y_test, y_test_pred)\n",
    "    \n",
    "    # Performing cross-validation for precision\n",
    "    precision_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='precision_weighted')\n",
    "    mean_precision = precision_scores.mean()\n",
    "    std_precision = precision_scores.std()\n",
    "    \n",
    "    # Performing cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    mean_accuracy = accuracy_scores.mean()\n",
    "    std_accuracy = accuracy_scores.std()\n",
    "    \n",
    "    results4.append((balance_name, \"Gradient Boosting\", mean_precision, std_precision, mean_accuracy, std_accuracy))\n",
    "    \n",
    "    print(f\"Mean Precision (CV): {mean_precision:.4f}\")\n",
    "    print(f\"Std Precision (CV): {std_precision:.4f}\")\n",
    "    print(f\"Mean Accuracy (CV): {mean_accuracy:.4f}\")\n",
    "    print(f\"Std Accuracy (CV): {std_accuracy:.4f}\\n\")\n",
    "\n",
    "# Compiling results in a DataFrame\n",
    "results4_df = pd.DataFrame(results4, columns=['Balancing', 'Classifier', 'Mean Precision', 'Std Precision', 'Mean Accuracy', 'Std Accuracy'])\n",
    "print(results4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SMOTE had better results with a mean precision of 0.855822 and a mean accuracy of 0.820168 compared to other balancing methods for the Gradient Boosting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Gradient Boosting\n",
    "results5 = []\n",
    "\n",
    "for balance_name, sampler in balancing_methods.items():\n",
    "    print(f\"Evaluating Naive Bayes with {balance_name} balancing method...\\n\")\n",
    "    \n",
    "    if sampler:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "    \n",
    "    \n",
    "    # Training and evaluating on test set\n",
    "    train_precision, test_precision, y_test_pred = evaluate_classifier(best_classifier, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    plot_metrics(\"Naive Bayes\", y_test, y_test_pred)\n",
    "    \n",
    "    # Performing cross-validation for precision\n",
    "    precision_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='precision_weighted')\n",
    "    mean_precision = precision_scores.mean()\n",
    "    std_precision = precision_scores.std()\n",
    "    \n",
    "    # Performing cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    mean_accuracy = accuracy_scores.mean()\n",
    "    std_accuracy = accuracy_scores.std()\n",
    "    \n",
    "    results5.append((balance_name, \"Naive Bayes\", mean_precision, std_precision, mean_accuracy, std_accuracy))\n",
    "    \n",
    "    print(f\"Mean Precision (CV): {mean_precision:.4f}\")\n",
    "    print(f\"Std Precision (CV): {std_precision:.4f}\")\n",
    "    print(f\"Mean Accuracy (CV): {mean_accuracy:.4f}\")\n",
    "    print(f\"Std Accuracy (CV): {std_accuracy:.4f}\\n\")\n",
    "\n",
    "# Compiling results in a DataFrame\n",
    "results5_df = pd.DataFrame(results5, columns=['Balancing', 'Classifier', 'Mean Precision', 'Std Precision', 'Mean Accuracy', 'Std Accuracy'])\n",
    "print(results5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SMOTE had better results with a mean precision of 0.858534 and a mean accuracy of 0.831765 compared to other balancing methods for the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating AdaBoost\n",
    "results6 = []\n",
    "\n",
    "for balance_name, sampler in balancing_methods.items():\n",
    "    print(f\"Evaluating AdaBoost with {balance_name} balancing method...\\n\")\n",
    "    \n",
    "    if sampler:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "\n",
    "    grid_search = GridSearchCV(AdaBoostClassifier(algorithm='SAMME'), param_grid=grid_params[\"AdaBoost\"], scoring='accuracy', cv=5)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    best_classifier = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Training and evaluating on test set\n",
    "    train_precision, test_precision, y_test_pred = evaluate_classifier(best_classifier, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(f\"Train Precision: {train_precision:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    \n",
    "    plot_metrics(\"AdaBoost\", y_test, y_test_pred)\n",
    "    \n",
    "    # Performing cross-validation for precision\n",
    "    precision_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='precision_weighted')\n",
    "    mean_precision = precision_scores.mean()\n",
    "    std_precision = precision_scores.std()\n",
    "    \n",
    "    # Performing cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(best_classifier, X_resampled, y_resampled, cv=5, scoring='accuracy')\n",
    "    mean_accuracy = accuracy_scores.mean()\n",
    "    std_accuracy = accuracy_scores.std()\n",
    "    \n",
    "    results6.append((balance_name, \"AdaBoost\", mean_precision, std_precision, mean_accuracy, std_accuracy))\n",
    "    \n",
    "    print(f\"Mean Precision (CV): {mean_precision:.4f}\")\n",
    "    print(f\"Std Precision (CV): {std_precision:.4f}\")\n",
    "    print(f\"Mean Accuracy (CV): {mean_accuracy:.4f}\")\n",
    "    print(f\"Std Accuracy (CV): {std_accuracy:.4f}\\n\")\n",
    "\n",
    "# Compiling results in a DataFrame\n",
    "results6_df = pd.DataFrame(results6, columns=['Balancing', 'Classifier', 'Mean Precision', 'Std Precision', 'Mean Accuracy', 'Std Accuracy'])\n",
    "print(results6_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SMOTE had better results with a mean precision of 0.871511 and a mean accuracy of 0.820504 compared to other balancing methods for the AdaBoost classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames\n",
    "all_results_df = pd.concat([results1_df, results2_df, results3_df, results4_df, results5_df, results6_df])\n",
    "\n",
    "# Reset index and start from 1\n",
    "all_results_df.reset_index(drop=True, inplace=True)\n",
    "all_results_df.index = all_results_df.index + 1\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(all_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.DataFrame({\n",
    "    'Balancing': [\n",
    "        'None', 'Under-sampling', 'SMOTE', 'ADASYN',\n",
    "        'None', 'Under-sampling', 'SMOTE', 'ADASYN',\n",
    "        'None', 'Under-sampling', 'SMOTE', 'ADASYN',\n",
    "        'None', 'Under-sampling', 'SMOTE', 'ADASYN',\n",
    "        'None', 'Under-sampling', 'SMOTE', 'ADASYN',\n",
    "        'None', 'Under-sampling', 'SMOTE', 'ADASYN'\n",
    "    ],\n",
    "    'Classifier': [\n",
    "        'Decision Tree', 'Decision Tree', 'Decision Tree', 'Decision Tree',\n",
    "        'K-Nearest Neighbors', 'K-Nearest Neighbors', 'K-Nearest Neighbors', 'K-Nearest Neighbors',\n",
    "        'Random Forest', 'Random Forest', 'Random Forest', 'Random Forest',\n",
    "        'Gradient Boosting', 'Gradient Boosting', 'Gradient Boosting', 'Gradient Boosting',\n",
    "        'Naive Bayes', 'Naive Bayes', 'Naive Bayes', 'Naive Bayes',\n",
    "        'AdaBoost', 'AdaBoost', 'AdaBoost', 'AdaBoost'\n",
    "    ],\n",
    "    'Mean Precision': [\n",
    "        0.585129, 0.595180, 0.729394, 0.715118,\n",
    "        0.644376, 0.590119, 0.675738, 0.679572,\n",
    "        0.583558, 0.694524, 0.884804, 0.868511,\n",
    "        0.687385, 0.632222, 0.855822, 0.810854,\n",
    "        0.682726, 0.608214, 0.858534, 0.814208,\n",
    "        0.673444, 0.733333, 0.871511, 0.840535\n",
    "    ],\n",
    "    'Mean Accuracy': [\n",
    "        0.619000, 0.557143, 0.698319, 0.722857,\n",
    "        0.711000, 0.585714, 0.663361, 0.670420,\n",
    "        0.702333, 0.700000, 0.831933, 0.850084,\n",
    "        0.693667, 0.600000, 0.820168, 0.792437,\n",
    "        0.709667, 0.628571, 0.831765, 0.792437,\n",
    "        0.710667, 0.728571, 0.820504, 0.798319\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Melt the dataframe to have a long format suitable for seaborn\n",
    "melted_df = pd.melt(all_results_df, id_vars=['Balancing', 'Classifier'], value_vars=['Mean Accuracy'])\n",
    "\n",
    "# Create a grouped bar chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Classifier', y='value', hue='Balancing', data=melted_df, ci=None)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('Comparison of Mean Accuracy for Different Classifiers and Balancing Methods')\n",
    "plt.legend(title='Balancing Method')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, after comprehensive evaluation of various classifiers and balancing methods, the Random Forest classifier, particularly when augmented with SMOTE or ADASYN for handling class imbalance, emerged as the top-performing model, consistently exhibiting the highest mean precision and mean accuracy scores across the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we developed six distinct Machine Learning models to determine which one was the most suitable for our problem and dataset. The findings indicated that Random Forest with ADASYN consistently achieved the highest mean accuracy and mean precision scores overall. We can also find that overall machine learning models have a better mean accuracy and mean precision when combined with the balancing methods SMOTE and ADASYN. \n",
    "We are satisfied with the models and the outcomes we obtained. This work helped us gain a better understanding of the workflow of a data science and machine learning project, as well\n",
    "as improve our knowledge of various Python data science libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree classifier, when paired with SMOTE and ADASYN balancing techniques, demonstrates improved performance in mean precision and mean accuracy, highlighting the effectiveness of synthetic oversampling methods in mitigating class imbalance. Similarly, K-Nearest Neighbors (KNN) benefits from oversampling methods, indicating enhanced capture of underlying data structure. In contrast, Random Forest consistently outperforms other classifiers across all balancing methods, attributed to its ensemble approach and robustness to overfitting, particularly accentuated when combined with SMOTE and ADASYN. Gradient Boosting also exhibits strong performance, especially with oversampled data, leveraging its iterative nature to focus on challenging instances. While Naive Bayes shows mixed results, AdaBoost consistently performs well across all methods, with SMOTE and ADASYN yielding the highest precision and accuracy. Recommendations for future analysis include further exploring hyperparameter tuning, ensemble methods, and feature engineering techniques to improve model performance. Overall, the study emphasizes the significance of addressing class imbalance and employing suitable preprocessing methods to enhance machine learning model efficacy, necessitating continued refinement for robust predictive models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
